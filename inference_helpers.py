"""
Author: Qianxi Li
Date: June 2, 2024
Description:
    This module provides functionalities for feedback inference and a major 
    voting mechanism on multiple generated sequences. It includes:
        1. major_vote_response:
            - Encodes a list of text responses.
            - Performs dimensionality reduction (PCA).
            - Detects outliers (IsolationForest).
            - Clusters the valid data (KMeans).
            - Returns the most centered response within the valid cluster.
        2. feedback_inference:
            - Uses a text-generation pipeline to generate feedback for predictions.
            - Applies major vote response selection for multiple returned sequences.
            - Aggregates and logs results.
"""

import transformers
import torch
import os
import tqdm
import json
import time
import numpy as np
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
from utils import log_method, ClearCache
from sklearn.impute import SimpleImputer


def major_vote_response(model, tokenizer, responses, contamination, batch_size):
    """
    major_vote_response():
        Takes multiple text responses generated by a model and:
            1) Encodes them into vector embeddings using the given model/tokenizer.
            2) Reduces dimensionality with PCA.
            3) Uses IsolationForest to filter out outliers.
            4) Uses KMeans (with one cluster) to find a central data point.
            5) Returns the most centered text response among the valid points.

    Args:
        model (torch.nn.Module): A language or embedding model used for encoding.
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer compatible with 'model'.
        responses (list): List of string responses to evaluate.
        contamination (float): The proportion of outliers in the data.
        batch_size (int): Batch size for encoding responses.

    Returns:
        tuple:
            - selected (str): The most centered response after filtering.
            - outliers (numpy.ndarray): The 2D points identified as outliers.
            - response_vectors_2d (numpy.ndarray): Dimensionality-reduced embeddings of all responses.
            - cluster_center (numpy.ndarray): Cluster center in 2D space.
            - data_center (numpy.ndarray): The 2D coordinates of the most centered point.
            - pure_vectors (numpy.ndarray): The 2D points that were not filtered as outliers.
    """
    def find_most_centered_data(center, reduced_vectors):
        # Compute Euclidean distances between the cluster center and each vector.
        distances = np.linalg.norm(reduced_vectors - center, axis=1)
        # Return the index of the vector closest to the center.
        idx = np.argmin(distances)
        return idx

    def batch_encode_strings(bert, tokenizer, strings, batch_size):
        # Evaluate the model to disable dropout, etc.
        model = bert
        model.eval()

        # Initialize a list to store embedding vectors.
        vectors = []

        # Process the strings in specified batch sizes.
        for i in range(0, len(strings), batch_size):
            batch = strings[i:i + batch_size]
            inputs = tokenizer(batch, padding=True, truncation=True,
                               return_tensors="pt", max_length=512)

            # Move input tensors to CUDA device if available.
            input_ids = inputs['input_ids'].to("cuda:0")
            attention_mask = inputs['attention_mask'].to("cuda:0")

            # Obtain hidden states from the model.
            with torch.no_grad():
                outputs = model(input_ids, attention_mask=attention_mask)
                hidden_states = outputs.last_hidden_state

            # Compute the mean embedding for each sequence in the batch.
            batch_vectors = hidden_states.mean(dim=1)
            vectors.extend(batch_vectors)

        # Stack all vectors into a single CPU-based tensor.
        vectors = torch.stack(vectors).cpu()
        return vectors

    # Encode all responses to obtain their vector representations.
    response_vectors = batch_encode_strings(model, tokenizer, responses, batch_size)

    # Handle potential missing values in embeddings (if any) using SimpleImputer.
    imputer = SimpleImputer(strategy='mean')
    response_vectors = imputer.fit_transform(response_vectors)

    # Apply PCA for dimensionality reduction to 2D.
    pca = PCA(n_components=2)
    response_vectors_2d = pca.fit_transform(response_vectors)

    # Detect outliers using IsolationForest.
    iso_forest = IsolationForest(contamination=contamination)
    outliers = iso_forest.fit_predict(response_vectors_2d)

    # Separate valid (inliers) and invalid (outliers) indices.
    valid_indices = [i for i, x in enumerate(outliers) if x == 1]
    invalid_indices = [i for i, x in enumerate(outliers) if x == -1]

    # Subset the 2D vectors and responses to only valid points.
    pure_vectors = response_vectors_2d[valid_indices]
    outliers = response_vectors_2d[invalid_indices]
    responses = [responses[i] for i in valid_indices]

    # Perform KMeans clustering with a single cluster (n_clusters=1).
    kmeans = KMeans(n_clusters=1, n_init='auto')
    cluster_labels = kmeans.fit_predict(pure_vectors)

    # Identify the cluster center (for the single cluster).
    cluster_center = kmeans.cluster_centers_[0]

    # Find the index of the most centered data point.
    selected_idx = find_most_centered_data(cluster_center, pure_vectors)
    data_center = pure_vectors[selected_idx]
    selected = responses[selected_idx]

    # Return the selected (most centered) response and diagnostic values.
    return selected, outliers, response_vectors_2d, cluster_center, data_center, pure_vectors


@log_method
def feedback_inference(model,
                       tokenizer,
                       feedback_prompt_data,
                       new_example_indices_dict,
                       num_return_seq,
                       bert_model,
                       bert_tokenizer,
                       contamination,
                       debug):
    """
    feedback_inference():
        Uses a text-generation pipeline to generate multiple feedback responses 
        for each prompt. For each set of returned responses:
            1) If multiple sequences are returned, selects the best using 
               'major_vote_response'.
            2) Stores the result in 'feedback_data'.
            3) Gathers selected feedback as potential new examples.

    Args:
        model (torch.nn.Module): A pre-trained or fine-tuned model for feedback generation.
        tokenizer (transformers.PreTrainedTokenizer): Tokenizer compatible with 'model'.
        feedback_prompt_data (dict): Dictionary containing prompts for feedback generation.
        new_example_indices_dict (dict): Maps tasks to indices where new examples should be extracted.
        num_return_seq (int): Number of feedback responses to generate per prompt.
        bert_model (torch.nn.Module): Model used for embedding in major vote response.
        bert_tokenizer (transformers.PreTrainedTokenizer): Tokenizer for 'bert_model'.
        contamination (float): Proportion of outliers in the isolation forest.
        debug (bool): If True, bypasses actual pipeline calls for a faster debugging cycle.

    Returns:
        tuple:
            - feedback_data (dict): Updated dictionary with feedback labels added.
            - prompt_example_dict (dict): Dictionary of selected feedback examples.
            - major_voting_log (list): A log list of diagnostic info for the major voting procedure.
    """
    # Clear GPU cache to manage memory usage.
    with ClearCache():
        # Initialize a text-generation pipeline from the model/tokenizer.
        pipeline = transformers.pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            device_map="auto",
            max_new_tokens=80,
            do_sample=True,
            num_return_sequences=num_return_seq
        )
        # Ensure the model uses the EOS token as padding if needed.
        pipeline.tokenizer.pad_token_id = pipeline.model.config.eos_token_id

        # Prepare storage for overall results and logs.
        result = []
        prompt_example_dict = {}
        log_counter = 0
        major_voting_log = []

        # Retrieve the full feedback prompt data.
        feedback_data = feedback_prompt_data

        # Iterate over each task in the feedback dataset.
        for task_name in tqdm.tqdm(list(feedback_data.keys()), desc="Each task fb generation:", position=0):
            # Filter out only keys that look like JSON tasks.
            if ".json" in task_name:
                task_dict = feedback_data[task_name]
                selected_example_index_list = new_example_indices_dict[task_name]
                index = 0
                prompt_example_list = []

                # Iterate over each feedback prompt in the current task.
                for each_feedback_prompt in task_dict["Feedback Prediction Prompt Dataset"]:
                    # Prepare a placeholder for the selected or single feedback.
                    truncated_result = "feedback_fake"

                    # If not in debug mode, actually run the text-generation pipeline.
                    if not debug:
                        res = pipeline(each_feedback_prompt)
                        truncated_result = [
                            res[i]['generated_text'][len(each_feedback_prompt):].split('\n\n')[0].strip()
                            for i in range(len(res))
                        ]

                        # If only one feedback is returned, use it; otherwise apply major vote.
                        if len(truncated_result) == 1:
                            result.append(truncated_result[0])
                            voted_feedback = truncated_result
                        else:
                            (voted_feedback,
                             outliers,
                             response_vectors_2d,
                             cluster_center,
                             data_center,
                             pure_vector2d) = major_vote_response(
                                bert_model,
                                bert_tokenizer,
                                truncated_result,
                                contamination=contamination,
                                batch_size=num_return_seq
                            )
                            result.append(voted_feedback)

                            # Log major voting details for up to 20 prompts.
                            if log_counter < 20:
                                tmp = {
                                    "each_feedback_prompt": each_feedback_prompt,
                                    "truncated_result": truncated_result,
                                    "outliers": outliers.tolist(),
                                    "response_vectors_2d": response_vectors_2d.tolist(),
                                    "cluster_center": cluster_center.tolist(),
                                    "data_center": data_center.tolist(),
                                    "pure_vector2d": pure_vector2d.tolist()
                                }
                                major_voting_log.append(tmp)
                                log_counter += 1

                    # If the current index is in the list of new examples, collect that feedback.
                    if index in selected_example_index_list:
                        prompt_example_list.append({
                            "input": task_dict['Instances'][index]["input"],
                            "output": task_dict['Instances'][index]["answer_prediction"],
                            "reason": voted_feedback
                        })

                    index += 1

                # Update the dictionary with the selected examples for this task.
                prompt_example_dict[task_name] = prompt_example_list

        # Attach the aggregated feedback to the data.
        feedback_data["Feedback Label"] = result

        # Delete the pipeline to free resources.
        del pipeline

    # Return the updated feedback data, the dictionary of new prompt examples, and logs.
    return feedback_data, prompt_example_dict, major_voting_log
